Imported from https://docs.google.com/document/d/1oP9-69yRPEUZw7UE8KKtB4547rJq7GTPjG9gmqC1MA4/edit#

=== Introduction
Last time on Cucumber School, we showed you how to use Example Mapping to facilitate conversations between stakeholders before starting work on a new user story.
We finished the lesson on a technical note, showing you how to extract a layer of support code from your step definitions to keep your Cucumber code easy - and cost-effective - to maintain.
We’re going to keep things technical in this lesson. Remember that bug we spotted right back at the beginning of lesson seven, where the user was over-charged if they mentioned “buy” several times in the same message? It’s finally time to knuckle down and fix it.
As we do so, you’re going to get some more experience of the inner and outer TDD loops that we first introduced you to in lesson 5. We’ll learn about mock objects, and explore the difference between unit tests and acceptance tests, and learn the value of each.
If you’re someone who doesn’t normally dive deep into code, try not to worry. We’ll keep the example from moving too fast, and we hope you’ll find it educational to see how different kinds of tests complement each other in helping you to build a quality product.

=== Tidy up the bug report Gherkin
Let’s work outside-in and start by tidying up the Gherkin specification. Right now, the scenario is still in the raw form it was in when the bug was first reported, with a name that references an ID in our bug tracking system. This doesn’t make for very good documentation about the intended behaviour.
Using our Friends Episode naming convention that we introduced in lesson 7, we can come up with a more expressive name for this scenario.
How about: the one where Sean mentions “buy” several times in the same shout?
[changes the scenario title]
You might be worried about losing this bug ID. We could keep it in a comment, a tag, or in the description of the Scenario if we wanted to. If your team likes to keep track of such things, feel free to do that. We’d prefer to consign the bug to history - what matters to us is how the software behaves today, not how it got to be that way.
We think the values in the example could be changed to make it a little more expressive. If we start Sean off with 100 credits, and end him with 95, it more clearly illustrates the rule that only five credits should be deducted.
[changes the scenario body]
Talking of which, let’s also update the rule to make it explicit that you lose 5 credits per shout.
RED: Improve failing scenario content and related rule

=== Hunt for the bug
Good. Now let’s run our improved scenario, using the @todo tag, and go hunting for that bug:
[runs cucumber --tags @todo]
OK, well we can see our scenario failing, but where do we need to go to fix it?
Luckily, our system is pretty small, and we remember that all the premium account behaviour is implemented in the Network class, so let’s go and look there.
[opens the Network class]
Well, that broadcast method is pretty complex, but I suppose the bug must be in there somewhere. Let’s try to pin it down with a unit test.
[opens the unit tests for Network]
Oh dear, oh dear: This file does not contain any tests for the premium account behaviour!
It looks like when that so-called hot-shot ninja rockstar subcontractor, Stevie, hacked in the first version of premium accounts, he drove everything from that one great big Cucumber scenario, and never wrote any unit tests.

=== The value of unit tests
[GoAnimate]
Why is this a problem?
Think about each automated test you write as warning light that you’re fitting to your system. Acceptance tests are warning lights that make sense to business people: when they fail, they tell you what functionality a user will not be able to enjoy because of the bug.
What a unit test does is give the programmers an indication about why the bug has occurred. Ideally, whenever you see an acceptance test’s warning light flash, there should be at least one corresponding unit test flashing too, pointing the developers to the source of the problem.
If you don’t have any unit tests, you’re left guessing where the problem lies. In a big system, this can be a serious waste of time.

=== Retrofit unit tests for existing behaviour
So the responsible thing to do at this point is to retrofit some unit tests for the Network class.  We’ll start with its existing behaviour, then go on to test-drive the behaviour we need to fix our bug.
Let’s write a test case for deducting five credits when the word “buy” is mentioned once.
All our existing tests use a test double or mock object to represent the instances of Person that the Network collaborates with. So let’s go with that and try to do the same here.
First we need a mock object to represent Sean.
[create test double for Sean]
And we’ll need to broadcast a message containing the word “buy”.
[write code to broadcast a message]
What next?
Well we need to assert that Sean is told to reduce his credits. In the deduct_credits method here, we’re calling the credits setter, setting it to five less than the current value. So we need to stub the attribute getter with, say 100 credits, and then check that the setter is called with 95. 
Here’s how we do that.
[writes the ugly stub & mock assertion]
RED: Add missing unit test for credit deductions by Network
<java>
This test is a real pain to write
</java>
<ruby>
This test is a real pain to write, and we’re still not done! 
Now it’s failing with this error. 
[run the tests, highlight the error message caused by listeners being nil / null]
Following the stack trace, it looks like the problem is that our list of listeners hasn’t been initialised. The Network class has been built with the assumption that there’s always going to be at least one listener subscribed.
So even though it’s not relevant to our test, we’ll need to create a Lucy, and subscribe her to the Network. She’ll also need to be ready to receive a call to hear when the message is broadcast.
Fix spec
</ruby>

=== The purpose of mock objects
Phew. Our test is passing at last. That was hard work, and what we’ve produced is just awful.
This ugly test is typical of what happens when you use mock objects or test doubles to retro-fit tests to existing code.
So why use mocks?
[break to GoAnimate]
Mock objects are a design tool. They’re intended to be used when constructing new code, to throw together a lightweight sketch of a collaborating object. When you’re focused on building a Network, and you think it needs to collaborate with a Person, it would slow you down to have to stop and go off to build the actual Person class. It’s often quicker to use a mock object to hash out your idea of roughly what Person might look like, leaving you to stay focused on the task at hand.
As you do this, you’ll find yourself - as we just did - having to specify how you want the two objects to interact. If you think about it, this interaction - the way they talk to each other at runtime - is the true behaviour of these objects.
Remember back in lesson 5, Loops, where we explained that acceptance tests help you build the right thing, and unit tests help you build the thing right? In other words, acceptance tests help us to explore the problem domain, and unit tests help us to explore the solution.
When you drive your development from tests, the unit tests can give you feedback about the design of your solution. If it’s easy to take an individual object and plug it into your tests, your design is likely to be nicely modular, which means it will be easier to change in the future. If it’s awkward, that’s a signal there’s a problem with your design.
We call this listening to the tests.

=== Listening to the tests
One alternative to using mocks would be to use a real instance of Person instead of the mock object. 
<ruby>
Alternate approach: Add missing unit test for credit deductions by Network - [show that integration test and then revert back to previous one]
This has a couple of apparent advantages. First, Person automatically subscribes to the Network as it is constructed, so we wouldn’t need this clutter about Lucy. Also, because we’re testing Person and Network as one integrated lump, we don’t have to fuss about exactly how they communicate about Sean’s credits. All we have to do is check how many credits he has left over at the end.
</ruby>
<java>
This is very tempting. By testing Person and Network as one integrated lump, we don’t have to fuss about exactly how they communicate about Sean’s credits. All we have to do is check how many credits he has left over at the end.
</java>
But this seductive solution papers over the cracks in our design.
When you use mocks, you put the microscope on the interaction between an object and its collaborators. This gives you  feedback about how coupled those objects are. A lot of chatter going back and forth like this suggests they’re getting tangled up together.
So the fact that it’s awkward to use mocks here is not because mocks are bad, or because we’re bad at using mocks: it’s feedback from the code.
Let’s listen to the tests and try to respond to this pain instead of ignoring it.
<ruby>

=== When nobody is listening
We’ll start with something easy and deal with the situation where the Network is asked to broadcast when it has no listeners. This is an annoying detail about how this object behaves that didn’t show itself in our acceptance tests. Let’s test-drive a fix.
[write a test for broadcasting to a network with no listeners, watch it fail]
Isolate a small deficiency in Network's contract.
To fix it, all we need to do is initialize the collection of listeners in the constructor, which makes more sense anyway.
[fix the code, watch the test pass]
Fix the bug when there are no listeners
Easy! And with that fixed, we can simplify the test we just wrote.
[remove Lucy from the 5 credits for mentioning buy unit test]
Now we can clean up the test.
</ruby>

=== Organising the tests by responsibility
Let’s take stock a little bit. We’re trying to fix that bug about multiple mentions of the word “buy”, but first we’re retrofitting unit tests for the premium account behaviour in the Network class.
It will be easier to see what tests are missing if we reorganize the unit tests by Network’s responsibilities.
They basically fall into two categories.
<ruby>
These ones here are to do with the responsibility of broadcasting to listeners, so let’s group them up using RSpec’s describe method.
</ruby>
<java>
These ones here are to do with the responsibility of broadcasting to listeners, so let’s group them in a different section of our test class.
</java>
Now we have our new test, which is about charging for shouts.
[puts that test in another describe block / section]

=== Split the obscure unit test into new, pending, independent tests
This last one here is a bit weird. It’s sort of about broadcasting to listeners, but it has this odd hack in it which needs further investigation [highlight the comment on the first line] . This test is quite obscure, and seems to be attempting to cover several aspects of Network’s behaviour all at once. Let’s document those individual behaviours as new tests and we can come back to deal with this one later.
Re-organise the unit tests for Person around responsibilities
<ruby>
We can see we’ll need something under the responsibility of broadcasting to listeners to ensure long messages aren’t broadcast for people with no credits, so we can express that using a pending RSpec example. We can fill this out later.
</ruby>
<java>
We can see we’ll need something similar under the responsibility of broadcasting to listeners, to ensure long messages aren’t broadcast for people with no credits, so we can express that using a test annotated with @Ignore for the time being. We can fill this out later.
</java>
[add pending/ignored example]
It’s clear we’ll also need to test that we charge two credits for an over-long message.
<ruby>Let’s add another pending example for that.</ruby>
<java>Let’s add another ignored test for that.</java>
[add pending example]
Write out two pending specs for the behaviour in the gnarly spec
Once those two tests are implemented, we won’t need this obscure one any more. We’re not confident enough to just remove it until we’ve seen those tests fleshed out, so we can mark it as <ruby>pending</ruby><java>ignore</java> for now. That will remind us we need to come back and clean this up soon.
Disable the gnarly test - we'll remove it soon

=== Decide to split Network’s responsibilities
At this point it is becoming clear that Network has too many responsibilities. We have two different sources of feedback telling us this:
With the unit tests laid out by responsibility, we can see more clearly that while broadcasting to listeners is a responsibility that fits, there’s no real reason why the Network needs be responsible for charging for shouts.
[GoAnimate sequence diagram of how a shout is currently done]
Secondly, the communication protocol between Network and Person, exposed by the mocks, is very noisy: we’re making lots of method calls to Person in order to determine whether they can afford the shout and to deduct payment:
First Sean calls Network#broadcast
then Network asks Sean how many credits he has
then Network tells Sean the new value of his credits, with the fee for the message deducted
then Network asks Sean how many credits he has again, in order to check whether he can afford to send the message!
We could get all sad and angry that if only we’d test-driven this code in the first place we might have got this feedback sooner, but that would be futile: it’s not too late to clean this code up!
How about we make Sean responsible for organising his own payment?

=== Beginning the refactoring
Instead of rewriting this code, let’s see if we can refactor it. When refactoring, it’s crucial not to break existing functionality, so we’ll rely on our tests to warn us if we make a mistake.
<ruby>
cucumber --tags ~@todo && rspec -f doc
</ruby>
<java>
mvn test -Dcucumber.options=”--tags ~@todo”
</java>
We’ll work in small steps. We may temporarily break some unit tests as the implementation shifts, but the acceptance tests should be passing all the way through if everything goes according to plan.
Ready? We’ll start by moving the deduct_credits method onto Person.
As a baby-step, we’ll make it public, so we can just continue to call it from Network’s broadcast method for now. That should keep the acceptance tests passing... Good.
[move method, run tests. cucumber passes, unit tests fail]
Now let’s make Sean responsible for deducting his own credits, as he shouts. We can move over this whole responsibility of charging for shouts from the network specs to the person specs.
[move charging for shouts tests from network tests to person tests file]
We’ll need to adjust the test like this… 
[amend test to use a real instance of Person, and assert on the value of credits]
Move `deduct_credits` onto Person
...and we’re back to green!
Green: move the call to deduct credits over to the shouter.
Let’s refactor some more.
Because all this code is running within the shouter object now, we don’t need to pass around this instance of Person anymore, and we can access the credits instance variable here, directly.
We can make deduct credits private now.
Make Person#deduct_credits private
We don’t need to pass short enough around as a variable, we can just do the logic inline here.
Refactor: inline `short_enough`

=== Clean up Person unit tests
Great, now the code in Network#broadcast is much cleaner.
We still have these pending unit tests we need to sort out. Let’s start in Person, by fleshing out this test about charging for long messages.
Retro-fit unit test for charging for long messages
That’s better. Let’s just make it fail quickly, to check that we can trust it.

=== Discuss the remaining Network unit tests
OK, now we can tackle these two <ruby>pending</ruby><java>ignored</java> unit tests in Network.
Let’s step back for a second and consider the rule we’re implementing here. This original unit test was simply there to ensure that long messages were not broadcast: remember we originally started out without premium accounts.
With the addition of premium accounts, the rule has changed: We will broadcast long messages, but only if the person shouting has sufficient credits.
To implement it, we’ve ended up with this mind-bending boolean logic in the broadcast method.  It doesn’t have to be like this. This complexity is a sign that we haven’t modelled the problem well enough yet.
Recall that right at the end of lesson 7, we discovered a known unknown in our problem domain and documented it with this question: what happens when you run out of credits? 
We know you can’t send a long message when you run out of credits, but what about a message containing the word ‘buy’? Perhaps having a complete answer to this question will help us to resolve this complexity.
Let’s check with our product owner.

=== Document the Rule about Running out of Credit
We use a scenario to frame the discussion with Paula, our product owner.
[Show the half-written scenario:]
Given Lucy and Sean are within range of each other - that’s what these background steps do - and Sean has bought 4 credits, when Sean shouts a message containing the word “buy” then… what?
Sean doesn’t have enough credits, so presumably Lucy does not hear his message, is that right?
Right, says Paula.
[writes the step]
And so in that case, Sean should still have his four credits, correct?
Right again, says Paula.
[writes the step]
OK, thanks Paula. So the rule is that we’ll only send shouts that you can afford. Is that right?
[writes the rule, deletes the question]
Correct! says Paula. A person can’t send a premium shout unless they have enough credits to pay for it.
Now we understand the business rule, let’s automate this scenario and drive out the behaviour.
Decide what happens when you run out of credit

=== Automate the scenario
First we need to match this new step where we check Lucy does not hear Sean’s message.
We have a very similar one here for Larry. We can use a capture group to make it more generic. Instead of a wildcard though, we can use pipes to specify the alternative values we expect here.
We capture the listener name, then use that here when fetching the instance of Person.
Make step more generic
Right, now we have a failing acceptance test which should be passing once we’ve resolved this. Let’s have a think about where this behaviour should go.
Only shout what you can afford: Failing unit test
If we give the responsibility for checking affordability to the Person doing the shouting, then Network doesn’t need to care about credits at all, which will simplify things nicely.
Let’s start with a new unit test for Person then.
We’ll set up Sean with only one credit - not enough to afford to send a long message - and assert that the broadcast method is not called on Network. Then we try to shout the long message.
This fails of course, because at the moment Person doesn’t make this check, so the broadcast method is called every time.
We’re at Red in the TDD cycle. Next stop: green!
COMMIT
Only shout what you can afford: Green
To make this pass we need to know how much the shout is going to cost before we send it. That knowledge is in the deduct_credits method, which actually does two tasks at the same time: it works out the cost of the message, and it deducts that cost from the person’s balance.
Let’s tease those two tasks apart, first calculating the cost of the shout, then deducting that cost from the credits.
Red: split responsibilities in Person#deduct_credits
Now we can extract a method that calculates the cost of the message.
Red: Extract method to calculate cost of message
Then finally we can use that method to check whether the message is affordable.
Green: Use cost_of to check message is affordable
...and our unit test is passing. Great.
And I suppose our acceptance test for the affordability rule should be passing now too. It is!

=== Refactor the tests
Refactor: Clean up dead tests and code for Network
We’re green, so it’s refactoring time again!   
Now we can give that messy old test in Network’s unit tests the treatment it deserves [deletes it] because this behaviour is now on Person. We won’t be needing this pending test.  [deletes it] <ruby> and we don’t need to stub credits here anymore. </ruby>
Refactor: delete redundant unit tests
Deleting code is my favourite kind of refactoring! Let’s keep going and strip back all that crazy boolean login in Network#broadcast. I think we can just remove this whole clause about the message being too short or having enough credits or whatever. Let’s try it and see what the tests tell us.
Refactor: remove dead code
Excellent, much better!

=== Refactor Network
Now, can we do any refactoring in Network?
This method would read a lot better if we extract a method that returns only the listeners within range of the shouter. Let’s do that.
[extracts method]
We don’t need to use this shouter_location temporary variable either. Let’s inline it.
Refactor: Extract method to simplify Network#broadcast

=== Closure
OK. The code is nice and clean, and all our tests are passing... except for one: that bug! It can’t escape us any longer. Let’s trap it with a unit test.
We know the responsibility for charging for shouts lies in Person, so that’s where to add the unit test.
Red: add unit test for the bug
Have you already spotted where we need to make the change? Here, in the cost_of(message) method, the code will add five credits to the cost each time it finds the word “buy” in the message. If we change the code like this, it should be working as expected.
Green: fix the bug when charging for "buy" several times
It is! Great stuff.

=== Epilogue
Although we’ve called this video series Cucumber School, you’ve probably noticed by now that we’re teaching you a whole range of skills and ideas that we on the Cucumber team use to develop software.
Not just skills with Cucumber, but skills with the whole of Behaviour-Driven Development.
BDD blurs the lines between traditional roles like tester and developer. In our ideal world, everyone on the team - not just testers - is responsible for quality, and we all try to test it as much as possible.
Having good automated tests frees up testers to do more interesting interactive, exploratory testing.
We use our tests as a guide to help us design a solution that models the problem well. A better model in your solution can make whole categories of bugs go away forever. We saw that in this lesson when we moved the responsibility for charging for shouts from Network to Person, and the issues around what happened when you run out of credit suddenly became easy to resolve.
Getting the right balance between fine-grained unit tests and broader full-stack tests is critically important to having fast, useful feedback from your test suite. We’ll explore this more in the next lesson.
See you next time, on Cucumber School.
